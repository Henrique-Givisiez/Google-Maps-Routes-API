### English Version & Vers√£o em Portugu√™s (ver mais abaixo)


# üöó Data Engineering Applied to Traffic

**How long does it take me to get to college?**  
What if I take another route? What‚Äôs the best time to leave?

Simple questions ‚Äî but for those who face traffic jams every day, intuition isn‚Äôt enough: you need data.  
This project demonstrates how **data engineering** can transform daily traffic chaos into **real insights**, applying APIs, pipelines, and data visualization.

## üß≠ Overview

This solution collects travel time estimates (**ETA**) for different routes using the **Google Maps Routes API**, compares **dynamic time (with traffic)** and **static time (without traffic)**, and stores everything in a **Lakehouse** for analysis in **Power BI**.

The goal was to **quantify the traffic I face every day** on my way between home, college, and work ‚Äî understanding variations, peaks, and route stability.

## üß© Solution Architecture

```mermaid
graph TD;
    A[Google Maps Routes API] --> B[PySpark Notebook];
    B --> C[Automated Ingestion Pipeline];
    C --> D[Lakehouse / Data Lake];
    D --> E[Power BI];
    E --> F[Dashboard and Insights];
```

## üîß Steps
1. Data Collection
    - Uses the Google Maps Routes API to capture:
      - Dynamic ETA (considering real-time traffic);
      - Static ETA (ignoring traffic conditions).
    - Data was collected every 10 minutes for 7 consecutive days via a Python notebook (`ingest-routes-api-data.ipynb`), scheduled through a Fabric pipeline.
2. Automated Ingestion
    - The notebook exports the data to a Delta Table and loads it into the Lakehouse.
3. Transformation
    - Data cleaning and calculations were done in Power Query:
        - Absolute difference between dynamic and static ETAs;
        - Percentage difference (real traffic impact).
3. Analysis and Visualization
    - In Power BI, data was transformed into visual indicators:
        - Executive KPIs;
        - Route comparisons;
        - Trend narratives throughout the day.

üìö Technologies Used
| Layer          | Tools / Technologies                    |
| :------------- | :-------------------------------------- |
| Collection     | Google Maps Routes API, Python, PySpark |
| Orchestration  | Fabric Pipeline (Scheduling)            |
| Storage        | Lakehouse (Delta Table)                 |
| Transformation | Power Query (Power BI)                  |
| Visualization  | Power BI Desktop and Service            |

# ‚öôÔ∏è Executions (Step-by-Step)
1. Get a Google Maps Routes API Key
    - Go to Google Cloud Console
    - Enable the Routes API and generate an API Key.

2. Set Variables in the Notebook
    - Replace YOUR_API_KEY with your actual key.
    - Adjust the origin/destination polylines using the script `utils/save_polylines.py`.
  
3. Run the Notebook

```bash
python ingest-routes-api-data.ipynb
```
4. Publish to the Lakehouse
    - Upload the generated files to your configured storage (e.g., Databricks, Azure, GCP, or AWS).

5. Connect Power BI
    - In Power BI Desktop ‚Üí Get Data ‚Üí Lakehouse / Parquet / CSV File
    - Perform transformations in Power Query and build dashboards.

# üöó Engenharia de Dados Aplicada ao Tr√¢nsito

**Quanto tempo eu demoro pra chegar na faculdade?**  
E se eu pegar outro caminho? Qual o melhor hor√°rio pra sair?

Perguntas simples - mas que, para quem enfrenta engarrafamentos todos os dias, exigem mais que intui√ß√£o: exigem dados.  
Este projeto mostra como a **engenharia de dados** pode transformar o tr√¢nsito do dia a dia em **insights reais**, aplicando APIs, pipelines e visualiza√ß√£o de dados.

## üß≠ Vis√£o Geral

A solu√ß√£o coleta estimativas de tempo de viagem (**ETA**) de diferentes rotas usando a **Google Maps Routes API**, compara o **tempo din√¢mico (com tr√°fego)** e o **tempo est√°tico (sem tr√°fego)**, e armazena tudo em um **Lakehouse** para an√°lise no **Power BI**.

O objetivo foi **metrificar o tr√¢nsito que eu enfrento diariamente** no trajeto entre casa, faculdade e trabalho - entendendo varia√ß√µes, picos e estabilidade das rotas.

## üß© Arquitetura da Solu√ß√£o

```mermaid
graph TD;
    A[Google Maps Routes API] --> B[Notebook PySpark];
    B --> C[Pipeline de Ingest√£o Automatizada];
    C --> D[Lakehouse / Data Lake];
    D --> E[Power BI];
    E --> F[Dashboard e Insights];
```

## üîß Etapas
1. Coleta de Dados
    - Utiliza a Google Maps Routes API para capturar:
        - ETA din√¢mico (considerando o tr√°fego atual);
        - ETA est√°tico (sem considerar o tr√°fego).
    - As coletas ocorreram a cada 10 minutos, por 7 dias consecutivos, via notebook Python (`ingest-routes-api-data.ipynb`) agendado por um pipeline do Fabric.
2. Ingest√£o Automatizada
    - O notebook exporta os dados para uma tabela delta e insere no Lakehouse.
3. Transforma√ß√£o
    - A limpeza e os c√°lculos s√£o feitos no Power Query:
        - Diferen√ßa absoluta entre ETA din√¢mico e est√°tico;
        - Diferen√ßa percentual (impacto real do tr√¢nsito).
4. An√°lise e Visualiza√ß√£o
    - No Power BI, os dados s√£o transformados em indicadores visuais:
        - KPIs executivos;
        - Compara√ß√µes entre rotas;
        - Narrativas de tend√™ncia ao longo dos hor√°rios do dia.

# üìö Tecnologias Utilizadas
| Camada        | Ferramentas / Tecnologias               |
| :------------ | :-------------------------------------- |
| Coleta        | Google Maps Routes API, Python, PySpark |s
| Orquestra√ß√£o  | Fabric Pipeline (agendamento)           |
| Armazenamento | Lakehouse (Delta Table)                 |
| Transforma√ß√£o | Power Query (Power BI)                  |
| Visualiza√ß√£o  | Power BI Desktop e Service              |

# ‚öôÔ∏è Execu√ß√£o (passo a passo)
1. Obter chave da Google Maps Routes API
    - Acesse Google Cloud Console
    - Habilite a Routes API e gere uma API Key.
2. Configurar vari√°veis no notebook
    - Substitua YOUR_API_KEY pela chave obtida.
    - Ajuste as polyines de origem/destino por meio do script `utils/save_polylines.py`
3. Executar o notebook

```bash
python ingest-routes-api-data.ipynb
```

4. Publica no Lakehouse
    - Envie os arquivos para o armazenamento configurado (ex: Databricks, Azure, GCP, AWS).

5. Conectar o Power BI
    - No Power BI Desktop ‚Üí Obter Dados ‚Üí Lakehouse / Arquivo Parquet / CSV
    - Realize as transforma√ß√µes no Power Query e monte os dashboards.